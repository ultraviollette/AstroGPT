import os
from typing import List, Any, Dict
import streamlit as st

from langchain.document_loaders import SitemapLoader
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document

# --- Cloudflare Product Sitemaps ---
CLOUDFLARE_SITEMAPS = [
    "https://developers.cloudflare.com/ai-gateway/sitemap.xml",
    "https://developers.cloudflare.com/vectorize/sitemap.xml",
    "https://developers.cloudflare.com/workers-ai/sitemap.xml",
]

# --- LLM and Embeddings Initialization (Dependent on API Key) ---
# api_key ì—ëŸ¬ê°€ ìê¾¸ ë‚˜ì„œ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ í˜¸ì¶œ ì‹œì ì— í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤

def get_llm():
    """Initializes and returns the ChatOpenAI instance, relying on the OPENAI_API_KEY environment variable."""
    return ChatOpenAI(
        temperature=0.1,
        model="gpt-4-turbo"
    )

def get_embeddings():
    """Initializes and returns the OpenAIEmbeddings instance, relying on the OPENAI_API_KEY environment variable."""
    return OpenAIEmbeddings()


# --- Prompts ---

answers_prompt = ChatPromptTemplate.from_template(
    """
    Using ONLY the following context answer the user's question. If you can't just say you don't know, don't make anything up.
                                                  
    Then, give a score to the answer between 0 and 5.

    If the answer answers the user question the score should be high, else it should be low.

    Make sure to always include the answer's score even if it's 0.

    Context: {context}
                                                  
    Examples:
                                                  
    Question: How far away is the moon?
    Answer: The moon is 384,400 km away.
    Score: 5
                                                  
    Question: How far away is the sun?
    Answer: I don't know
    Score: 0
                                                  
    Your turn!

    Question: {question}
"""
)

choose_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Use ONLY the following pre-existing answers to answer the user's question.

            Use the answers that have the highest score (more helpful) and favor the most recent ones.

            Cite sources and return the sources of the answers as they are, do not change them.

            Answers: {answers}
            """,
        ),
        ("human", "{question}"),
    ]
)

# --- Chain Logic Functions ---

def get_answers(inputs):
    """Generates answers for each retrieved document and includes source metadata."""
    docs: List[Document] = inputs["docs"]
    question: str = inputs["question"]
    llm = get_llm()
    answers_chain = answers_prompt | llm

    # ê° ë¬¸ì„œ ì¡°ê°ì— ëŒ€í•´ LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ê³¼ ì ìˆ˜ë¥¼ ìƒì„±
    return {
        "question": question,
        "answers": [
            {
                "answer": answers_chain.invoke(
                    {"question": question, "context": doc.page_content}
                ).content,
                "source": doc.metadata.get("source", "Unknown"),
                "date": doc.metadata.get("lastmod", "Unknown"),
            }
            for doc in docs
        ],
    }

def choose_answer(inputs):
    """Selects the best final answer from the generated answers based on score and recency."""
    answers: List[Dict[str, Any]] = inputs["answers"]
    question: str = inputs["question"]
    llm = get_llm()
    choose_chain = choose_prompt | llm
    
    # ì—¬ëŸ¬ ë‹µë³€ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì••ì¶• (Choose Promptì— ë§ê²Œ í¬ë§·íŒ…)
    condensed = "\n\n".join(
            f"Answer: {answer['answer']}\nSource: {answer['source']}\nDate: {answer['date']}"
            for answer in answers
    )
    
    # ìµœì¢… ë‹µë³€ ì„ íƒ LLM í˜¸ì¶œ
    return choose_chain.invoke(
        {
            "question": question,
            "answers": condensed,
        }
    )

def parse_page(soup):
    """Removes irrelevant elements (header, footer) from the page content."""
    # Cloudflare ë¬¸ì„œì— ë§ì¶° ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
    main_content = soup.find("main") or soup
    
    # ë¶ˆí•„ìš”í•œ ë„¤ë¹„ê²Œì´ì…˜ ë° í—¤ë” ì œê±° (SitemapLoaderê°€ ê°€ì ¸ì˜¨ ì „ì²´ HTMLì—ì„œ)
    for tag in main_content.find_all(['header', 'footer', 'nav', 'aside', 'script', 'style']):
        tag.decompose()
        
    return (
        str(main_content.get_text())
        .replace("\n", " ")
        .replace("\xa0", " ")
        .strip()
    )

@st.cache_resource(show_spinner="Loading Cloudflare documentation (may take a moment)...")
def load_website(api_key: str):
    """Loads, splits, and embeds all Cloudflare documentation sitemaps."""
    if not api_key:
        return None

    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=200,
    )
    
    all_docs = []
    
    # URL í•„í„°ë§ì„ ìœ„í•œ íŒ¨í„´ ì„¤ì •
    url_filters = [
        r"https://developers\.cloudflare\.com/ai-gateway/",
        r"https://developers\.cloudflare\.com/vectorize/",
        r"https://developers\.cloudflare\.com/workers-ai/",
    ]

    # 3ê°œì˜ sitemapì—ì„œ ë¬¸ì„œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¡œë“œ
    for url in CLOUDFLARE_SITEMAPS:
        st.write(f"Loading documentation from: {url}")
        try:
            loader = SitemapLoader(
                url, 
                parsing_function=parse_page,
                filter_urls=url_filters # URL í•„í„°ë§ ì ìš©
            )

            loader.requests_per_second = 15 
            docs = loader.load_and_split(text_splitter=splitter)
            all_docs.extend(docs)
        except Exception as e:
            st.warning(f"Failed to load documents from {url}: {e}")
            
    if not all_docs:
        st.error("Could not load any documentation. Please check the URLs or try again later.")
        return None

    st.write(f"Total {len(all_docs)} chunks loaded and ready for embedding.")
    
    # ì„ë² ë”© ìƒì„± (ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ì§€ ì•ŠìŒ)
    embeddings = get_embeddings()
    
    # ğŸŒŸ í† í° í•œë„ ì´ˆê³¼ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ë¬¸ì„œ ë°°ì¹˜ë¥¼ ë‚˜ëˆ„ì–´ ì„ë² ë”©í•©ë‹ˆë‹¤.
    # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°ë¥¼ 250ê°œ ë¬¸ì„œë¡œ ì„¤ì • (300k í† í° ì œí•œ ì´í•˜ë¥¼ ëª©í‘œ)
    BATCH_SIZE = 250 
    vector_store = None
    
    # Streamlit ì§„í–‰ í‘œì‹œì¤„ ì„¤ì •
    embedding_status = st.empty()
    embedding_progress = st.progress(0)
    
    for i in range(0, len(all_docs), BATCH_SIZE):
        batch = all_docs[i:i + BATCH_SIZE]
        current_progress = (i + len(batch)) / len(all_docs)
        
        embedding_status.info(f"Embedding batch {i // BATCH_SIZE + 1} of {len(all_docs) // BATCH_SIZE + 1}...")
        
        try:
            if vector_store is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜: FAISS ì €ì¥ì†Œ ì´ˆê¸°í™”
                vector_store = FAISS.from_documents(batch, embeddings)
            else:
                # í›„ì† ë°°ì¹˜: ê¸°ì¡´ ì €ì¥ì†Œì— ë¬¸ì„œ ì¶”ê°€
                vector_store.add_documents(batch)
            
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            embedding_progress.progress(current_progress)

        except Exception as e:
            st.error(f"Error during embedding batch {i // BATCH_SIZE + 1}: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì¤‘ë‹¨í•˜ê³  None ë°˜í™˜
            return None

    # ì„ë² ë”© ì™„ë£Œ ë©”ì‹œì§€
    embedding_status.success("Document embedding complete!")
    
    return vector_store.as_retriever()

# --- Streamlit UI and Execution ---

st.set_page_config(
    page_title="Cloudflare SiteGPT",
    page_icon="ğŸ–¥ï¸",
)

st.markdown(
    """
    # SiteGPT for Cloudflare's documentation
            
    Ask questions about the documentation for Cloudflare's **AI Gateway**, **Vectorize**, and **Workers AI**.
            
    The system will retrieve relevant context from the documentation and use it to provide grounded answers.
"""
)

# --- Sidebar ---
with st.sidebar:
    st.header("Settings")
    api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        help="Enter your OpenAI API key to run the application.",
        key="api_key_input"
    )
    st.markdown("---")
    st.markdown("[GitHub Repository Link](https://github.com/ultraviollette/AstroGPT)") 
    st.markdown("---")


# --- Main Application Logic ---

if not api_key:
    st.warning("Please enter your OpenAI API Key in the sidebar to start.")
else:
    # API í‚¤ë¥¼ ì „ì—­ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  LangChain ì»´í¬ë„ŒíŠ¸ê°€ ì‚¬ìš©í•˜ë„ë¡ ë³´ì¥
    os.environ["OPENAI_API_KEY"] = api_key
    
    # 1. ë¬¸ì„œ ë¡œë“œ ë° ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” (API í‚¤ í•„ìš”)
    retriever = load_website(api_key)
    
    if retriever:
        # 2. ì§ˆë¬¸ ì…ë ¥
        query = st.text_input("Ask a question about the Cloudflare documentation", key="query_input")
        
        if query:
            # 3. RAG ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
            
            chain = (
                {
                    "docs": retriever,
                    "question": RunnablePassthrough(),
                }
                | RunnableLambda(get_answers) # get_answersëŠ” ì´ì œ í™˜ê²½ ë³€ìˆ˜ì—ì„œ í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                | RunnableLambda(choose_answer) # choose_answerë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤.
            )
            
            with st.spinner("Searching and generating answer..."):
                try:
                    # ìµœì¢… ê²°ê³¼ í˜¸ì¶œ
                    result = chain.invoke(query)
                    
                    # Markdown ê²°ê³¼ í‘œì‹œ. $ ë¬¸ìê°€ LaTeX ìˆ˜ì‹ìœ¼ë¡œ í•´ì„ë˜ì§€ ì•Šë„ë¡ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                    st.markdown(result.content.replace("$", "\$"))
                    
                except Exception as e:
                    st.error(f"An error occurred during chain execution: {e}")
                    # API í‚¤ê°€ ì˜ëª»ëœ ê²½ìš°, ëª…í™•í•˜ê²Œ ì•ˆë‚´í•©ë‹ˆë‹¤.
                    st.info("Please check if your API key is correct and valid for the specified model.")
    else:
        st.error("Could not initialize the documentation retriever. Please ensure the API key is valid.")